#!/usr/bin/env python3
"""
Enhanced ML Prediction System with Comprehensive Data Pipeline Integration
Integrates with the advanced data pipeline for superior prediction accuracy
"""

import asyncio
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import json
import sqlite3

# Import our data pipeline
from data_integration_engine import DataIntegrationEngine, DataManager
from data_pipeline_api import get_enhanced_ml_prediction_data

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedMLPredictionEngine:
    """Enhanced ML prediction engine with comprehensive data pipeline integration"""
    
    def __init__(self, model_path: str = "goldgpt_enhanced_ml_models"):
        self.model_path = model_path
        self.models = {}
        self.scalers = {}
        self.feature_names = []
        self.training_history = []
        
        # Initialize data pipeline
        self.data_integration_engine = DataIntegrationEngine()
        self.data_manager = DataManager(self.data_integration_engine)
        
        # Model configurations
        self.model_configs = {
            'random_forest': {
                'model': RandomForestRegressor(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42
                ),
                'name': 'Random Forest',
                'prediction_horizon': '1h'
            },
            'gradient_boost': {
                'model': GradientBoostingRegressor(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=6,
                    random_state=42
                ),
                'name': 'Gradient Boosting',
                'prediction_horizon': '4h'
            }
        }
        
        # Initialize models
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize ML models and scalers"""
        for model_name, config in self.model_configs.items():
            self.models[model_name] = config['model']
            self.scalers[model_name] = StandardScaler()
    
    async def collect_training_data(self, days_back: int = 30) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Collect comprehensive training data from the data pipeline"""
        logger.info(f"Collecting training data for {days_back} days...")
        
        # Get unified dataset
        dataset = await self.data_manager.get_ml_ready_dataset(force_refresh=True)
        
        if not dataset or not dataset.get('features'):
            raise ValueError("No training data available from data pipeline")
        
        # For training, we need historical data points
        # In production, this would collect multiple time points
        # For now, we'll simulate historical data
        training_features = []
        training_targets = []
        
        # Get feature names
        feature_names = self.data_manager.get_feature_names(dataset)
        
        # Simulate multiple data points (in production, collect from database)
        current_features = self.data_manager.get_feature_vector(dataset)
        # Use Gold API price or reasonable fallback
        try:
            from price_storage_manager import get_current_gold_price
            current_price = dataset.get('features', {}).get('current_price', get_current_gold_price() or 3430.0)
        except:
            current_price = dataset.get('features', {}).get('current_price', 3430.0)
        
        # Generate synthetic training data based on current features
        # In production, this would be replaced with actual historical data collection
        for i in range(100):  # 100 synthetic data points
            # Add some noise and variation to current features
            noise_factor = 0.1
            synthetic_features = current_features + np.random.normal(0, noise_factor, current_features.shape)
            
            # Create synthetic target (price change) based on features
            # This is a simplified approach - in production use actual historical price changes
            price_momentum = synthetic_features[feature_names.index('current_price')] if 'current_price' in feature_names else 0
            sentiment_impact = synthetic_features[feature_names.index('news_sentiment_avg')] * 10 if 'news_sentiment_avg' in feature_names else 0
            
            synthetic_target = price_momentum * 0.001 + sentiment_impact + np.random.normal(0, 5)
            
            training_features.append(synthetic_features)
            training_targets.append(synthetic_target)
        
        X = np.array(training_features)
        y = np.array(training_targets)
        
        logger.info(f"Collected training data: {X.shape[0]} samples, {X.shape[1]} features")
        return X, y, feature_names
    
    async def train_models(self, days_back: int = 30) -> Dict[str, Any]:
        """Train all ML models with comprehensive data"""
        logger.info("Training enhanced ML models...")
        
        try:
            # Collect training data
            X, y, feature_names = await self.collect_training_data(days_back)
            self.feature_names = feature_names
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            training_results = {}
            
            for model_name, config in self.model_configs.items():
                logger.info(f"Training {config['name']} model...")
                
                # Scale features
                X_train_scaled = self.scalers[model_name].fit_transform(X_train)
                X_test_scaled = self.scalers[model_name].transform(X_test)
                
                # Train model
                self.models[model_name].fit(X_train_scaled, y_train)
                
                # Evaluate model
                train_pred = self.models[model_name].predict(X_train_scaled)
                test_pred = self.models[model_name].predict(X_test_scaled)
                
                # Calculate metrics
                train_mae = mean_absolute_error(y_train, train_pred)
                test_mae = mean_absolute_error(y_test, test_pred)
                train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
                train_r2 = r2_score(y_train, train_pred)
                test_r2 = r2_score(y_test, test_pred)
                
                model_results = {
                    'model_name': config['name'],
                    'prediction_horizon': config['prediction_horizon'],
                    'training_samples': len(X_train),
                    'test_samples': len(X_test),
                    'metrics': {
                        'train_mae': train_mae,
                        'test_mae': test_mae,
                        'train_rmse': train_rmse,
                        'test_rmse': test_rmse,
                        'train_r2': train_r2,
                        'test_r2': test_r2
                    },
                    'feature_importance': self._get_feature_importance(model_name),
                    'training_timestamp': datetime.now().isoformat()
                }
                
                training_results[model_name] = model_results
                logger.info(f"{config['name']} trained - Test MAE: {test_mae:.4f}, Test RÂ²: {test_r2:.4f}")
            
            # Save models
            await self._save_models()
            
            # Store training history
            self.training_history.append({
                'timestamp': datetime.now().isoformat(),
                'models_trained': list(training_results.keys()),
                'training_samples': len(X),
                'feature_count': len(feature_names),
                'results': training_results
            })
            
            logger.info("Enhanced ML models trained successfully")
            return training_results
            
        except Exception as e:
            logger.error(f"Model training failed: {e}")
            raise e
    
    def _get_feature_importance(self, model_name: str) -> Dict[str, float]:
        """Get feature importance from trained model"""
        model = self.models[model_name]
        
        if hasattr(model, 'feature_importances_'):
            importance_scores = model.feature_importances_
            feature_importance = {}
            
            for i, importance in enumerate(importance_scores):
                if i < len(self.feature_names):
                    feature_importance[self.feature_names[i]] = float(importance)
            
            # Sort by importance
            sorted_importance = dict(sorted(feature_importance.items(), 
                                         key=lambda x: x[1], reverse=True))
            return dict(list(sorted_importance.items())[:10])  # Top 10 features
        
        return {}
    
    async def generate_predictions(self, prediction_horizons: List[str] = None) -> Dict[str, Any]:
        """Generate comprehensive ML predictions using all trained models"""
        logger.info("Generating enhanced ML predictions...")
        
        if prediction_horizons is None:
            prediction_horizons = ['1h', '4h', '24h']
        
        try:
            # Get current data from pipeline
            dataset = await self.data_manager.get_ml_ready_dataset()
            current_features = self.data_manager.get_feature_vector(dataset)
            # Use Gold API price or reasonable fallback
            try:
                from price_storage_manager import get_current_gold_price
                current_price = dataset.get('features', {}).get('current_price', get_current_gold_price() or 3430.0)
            except:
                current_price = dataset.get('features', {}).get('current_price', 3430.0)
            
            predictions = {}
            
            for model_name, config in self.model_configs.items():
                if config['prediction_horizon'] in prediction_horizons:
                    
                    # Scale features
                    features_scaled = self.scalers[model_name].transform(current_features.reshape(1, -1))
                    
                    # Generate prediction
                    price_change_prediction = self.models[model_name].predict(features_scaled)[0]
                    predicted_price = current_price + price_change_prediction
                    
                    # Calculate confidence based on model performance
                    confidence = self._calculate_prediction_confidence(model_name, dataset)
                    
                    # Determine direction and strength
                    direction = 'bullish' if price_change_prediction > 0 else 'bearish'
                    strength = min(abs(price_change_prediction) / current_price * 100, 100)  # As percentage
                    
                    prediction_data = {
                        'model_name': config['name'],
                        'horizon': config['prediction_horizon'],
                        'current_price': current_price,
                        'predicted_price': predicted_price,
                        'price_change': price_change_prediction,
                        'price_change_percent': (price_change_prediction / current_price) * 100,
                        'direction': direction,
                        'strength': strength,
                        'confidence': confidence,
                        'timestamp': datetime.now().isoformat(),
                        'data_quality': dataset.get('data_quality', {}).get('overall_score', 0),
                        'key_factors': self._get_key_prediction_factors(model_name, current_features)
                    }
                    
                    predictions[model_name] = prediction_data
            
            # Create ensemble prediction
            ensemble_prediction = self._create_ensemble_prediction(predictions, current_price)
            predictions['ensemble'] = ensemble_prediction
            
            logger.info(f"Generated {len(predictions)} enhanced ML predictions")
            return predictions
            
        except Exception as e:
            logger.error(f"Prediction generation failed: {e}")
            return self._generate_fallback_predictions()
    
    def _calculate_prediction_confidence(self, model_name: str, dataset: Dict[str, Any]) -> float:
        """Calculate prediction confidence based on model performance and data quality"""
        
        # Base confidence from data quality
        data_quality_score = dataset.get('data_quality', {}).get('overall_score', 0.5)
        
        # Model performance factor (from training history)
        model_performance = 0.7  # Default, would use actual test performance
        if self.training_history:
            latest_training = self.training_history[-1]
            model_results = latest_training.get('results', {}).get(model_name, {})
            test_r2 = model_results.get('metrics', {}).get('test_r2', 0.5)
            model_performance = max(0.1, min(1.0, test_r2))
        
        # Feature completeness factor
        features = dataset.get('features', {})
        feature_completeness = len([v for v in features.values() if v is not None]) / len(features) if features else 0.5
        
        # Combined confidence
        confidence = (data_quality_score * 0.4 + model_performance * 0.4 + feature_completeness * 0.2)
        return round(confidence, 3)
    
    def _get_key_prediction_factors(self, model_name: str, features: np.ndarray) -> List[Dict[str, Any]]:
        """Get key factors influencing the prediction"""
        
        feature_importance = self._get_feature_importance(model_name)
        key_factors = []
        
        # Get top 5 most important features
        for feature_name, importance in list(feature_importance.items())[:5]:
            if feature_name in self.feature_names:
                feature_index = self.feature_names.index(feature_name)
                feature_value = features[feature_index] if feature_index < len(features) else 0
                
                key_factors.append({
                    'feature': feature_name,
                    'importance': importance,
                    'current_value': feature_value,
                    'impact': 'positive' if feature_value > 0 else 'negative' if feature_value < 0 else 'neutral'
                })
        
        return key_factors
    
    def _create_ensemble_prediction(self, predictions: Dict[str, Any], current_price: float) -> Dict[str, Any]:
        """Create ensemble prediction by combining individual model predictions"""
        
        if not predictions:
            return self._generate_fallback_predictions()['ensemble']
        
        # Weighted average based on confidence
        total_weight = 0
        weighted_price_change = 0
        weighted_confidence = 0
        
        valid_predictions = [p for p in predictions.values() if 'confidence' in p]
        
        for pred in valid_predictions:
            weight = pred['confidence']
            weighted_price_change += pred['price_change'] * weight
            weighted_confidence += pred['confidence'] * weight
            total_weight += weight
        
        if total_weight == 0:
            return self._generate_fallback_predictions()['ensemble']
        
        # Calculate ensemble values
        ensemble_price_change = weighted_price_change / total_weight
        ensemble_confidence = weighted_confidence / total_weight
        ensemble_predicted_price = current_price + ensemble_price_change
        
        # Determine ensemble direction and strength
        ensemble_direction = 'bullish' if ensemble_price_change > 0 else 'bearish'
        ensemble_strength = min(abs(ensemble_price_change) / current_price * 100, 100)
        
        return {
            'model_name': 'Ensemble',
            'horizon': 'multi_timeframe',
            'current_price': current_price,
            'predicted_price': ensemble_predicted_price,
            'price_change': ensemble_price_change,
            'price_change_percent': (ensemble_price_change / current_price) * 100,
            'direction': ensemble_direction,
            'strength': ensemble_strength,
            'confidence': ensemble_confidence,
            'timestamp': datetime.now().isoformat(),
            'models_used': len(valid_predictions),
            'prediction_method': 'confidence_weighted_ensemble'
        }
    
    def _generate_fallback_predictions(self) -> Dict[str, Any]:
        """Generate fallback predictions when main system fails"""
        
        fallback_predictions = {
            'random_forest': {
                'model_name': 'Random Forest (Fallback)',
                'horizon': '1h',
                'current_price': 2000.0,
                'predicted_price': 2001.5,
                'price_change': 1.5,
                'price_change_percent': 0.075,
                'direction': 'bullish',
                'strength': 25.0,
                'confidence': 0.3,
                'timestamp': datetime.now().isoformat(),
                'status': 'fallback'
            },
            'ensemble': {
                'model_name': 'Ensemble (Fallback)',
                'horizon': 'multi_timeframe',
                'current_price': 2000.0,
                'predicted_price': 2000.5,
                'price_change': 0.5,
                'price_change_percent': 0.025,
                'direction': 'neutral',
                'strength': 10.0,
                'confidence': 0.2,
                'timestamp': datetime.now().isoformat(),
                'status': 'fallback'
            }
        }
        
        return fallback_predictions
    
    async def _save_models(self):
        """Save trained models and scalers"""
        try:
            for model_name in self.models.keys():
                joblib.dump(self.models[model_name], f"{self.model_path}_{model_name}_model.pkl")
                joblib.dump(self.scalers[model_name], f"{self.model_path}_{model_name}_scaler.pkl")
            
            # Save feature names
            with open(f"{self.model_path}_feature_names.json", 'w') as f:
                json.dump(self.feature_names, f)
            
            logger.info("Models saved successfully")
        except Exception as e:
            logger.error(f"Failed to save models: {e}")
    
    async def load_models(self) -> bool:
        """Load pre-trained models"""
        try:
            for model_name in self.model_configs.keys():
                model_path = f"{self.model_path}_{model_name}_model.pkl"
                scaler_path = f"{self.model_path}_{model_name}_scaler.pkl"
                
                try:
                    self.models[model_name] = joblib.load(model_path)
                    self.scalers[model_name] = joblib.load(scaler_path)
                except FileNotFoundError:
                    logger.warning(f"Model {model_name} not found, will use untrained model")
            
            # Load feature names
            try:
                with open(f"{self.model_path}_feature_names.json", 'r') as f:
                    self.feature_names = json.load(f)
            except FileNotFoundError:
                logger.warning("Feature names file not found")
            
            return True
        except Exception as e:
            logger.error(f"Failed to load models: {e}")
            return False
    
    async def get_model_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive model performance report"""
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'models_available': len(self.models),
            'feature_count': len(self.feature_names),
            'training_history_count': len(self.training_history),
            'data_pipeline_status': 'active',
            'models': {}
        }
        
        # Get latest training results
        if self.training_history:
            latest_training = self.training_history[-1]
            for model_name, results in latest_training.get('results', {}).items():
                report['models'][model_name] = {
                    'performance': results.get('metrics', {}),
                    'feature_importance': results.get('feature_importance', {}),
                    'last_trained': results.get('training_timestamp'),
                    'prediction_horizon': results.get('prediction_horizon')
                }
        
        # Test data pipeline health
        try:
            health_status = await self.data_manager.health_check()
            report['data_pipeline_health'] = health_status
        except Exception as e:
            report['data_pipeline_health'] = {'status': 'error', 'error': str(e)}
        
        return report
    
    def cleanup(self):
        """Cleanup resources"""
        if self.data_integration_engine:
            self.data_integration_engine.close()

# Global instance
enhanced_ml_engine = None

async def get_enhanced_ml_predictions(horizons: List[str] = None) -> Dict[str, Any]:
    """Get enhanced ML predictions (main API function)"""
    global enhanced_ml_engine
    
    try:
        if enhanced_ml_engine is None:
            enhanced_ml_engine = EnhancedMLPredictionEngine()
            await enhanced_ml_engine.load_models()
        
        predictions = await enhanced_ml_engine.generate_predictions(horizons)
        
        return {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'predictions': predictions,
            'model_count': len(predictions),
            'data_pipeline_integrated': True
        }
    except Exception as e:
        logger.error(f"Enhanced ML prediction failed: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'fallback_predictions': enhanced_ml_engine._generate_fallback_predictions() if enhanced_ml_engine else {}
        }

async def train_enhanced_ml_models() -> Dict[str, Any]:
    """Train enhanced ML models (admin function)"""
    global enhanced_ml_engine
    
    try:
        if enhanced_ml_engine is None:
            enhanced_ml_engine = EnhancedMLPredictionEngine()
        
        training_results = await enhanced_ml_engine.train_models()
        
        return {
            'status': 'success',
            'timestamp': datetime.now().isoformat(),
            'training_results': training_results,
            'message': 'Enhanced ML models trained successfully'
        }
    except Exception as e:
        logger.error(f"Enhanced ML training failed: {e}")
        return {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

# Example usage
async def main():
    """Example usage of enhanced ML prediction system"""
    
    print("ð¤ GoldGPT Enhanced ML Prediction System")
    print("=" * 50)
    
    # Initialize engine
    engine = EnhancedMLPredictionEngine()
    
    try:
        # Train models
        print("ð¯ Training enhanced ML models...")
        training_results = await engine.train_models(days_back=30)
        print(f"â Training complete: {len(training_results)} models trained")
        
        # Generate predictions
        print("ð® Generating enhanced predictions...")
        predictions = await engine.generate_predictions(['1h', '4h'])
        print(f"â Generated {len(predictions)} predictions")
        
        # Display results
        for model_name, pred in predictions.items():
            print(f"\n{pred['model_name']} ({pred['horizon']}):")
            print(f"  Current Price: ${pred['current_price']:.2f}")
            print(f"  Predicted Price: ${pred['predicted_price']:.2f}")
            print(f"  Direction: {pred['direction']} ({pred['strength']:.1f}% strength)")
            print(f"  Confidence: {pred['confidence']:.3f}")
        
        # Performance report
        print("\nð Getting performance report...")
        report = await engine.get_model_performance_report()
        print(f"â Report generated: {report['models_available']} models, {report['feature_count']} features")
        
    except Exception as e:
        print(f"â Error: {e}")
    finally:
        engine.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
